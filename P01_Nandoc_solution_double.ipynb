{"cells":[{"cell_type":"markdown","metadata":{"id":"rG6hH59LyLVU"},"source":["\n","\n","---\n","# ë‚œë…í™” ë³µì› ì†”ë£¨ì…˜: ë”ë¸” ëª¨ë¸\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ry5hMfHlNATB","outputId":"c44b5e66-20f0-4935-fd6d-eba4604054f5","executionInfo":{"status":"ok","timestamp":1768451390485,"user_tz":-540,"elapsed":37543,"user":{"displayName":"SiHOON KIM","userId":"10710758819479501067"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2026.1.4)\n","Mounted at /content/drive\n","ğŸš€ Inference Device: cuda\n"]}],"source":["# =============================================================================\n","# 0. í™˜ê²½ ì„¤ì •\n","# =============================================================================\n","!pip install transformers\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import time\n","import re\n","import torch\n","from transformers import ElectraTokenizer, ElectraForSequenceClassification\n","from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration\n","\n","# =============================================================================\n","# 1. ê²½ë¡œ ë° í”„ë¡¬í”„íŠ¸ ì„¤ì •\n","# =============================================================================\n","# [Signal] ë¶„ë¥˜ê¸°\n","PATH_CLASSIFIER = \"/content/drive/MyDrive/P01_Nandoc/Classifier\"\n","\n","# [Core 1] ì˜¤íƒ€ ë‹´ë‹¹ (íŒ€ì›ì´ í•™ìŠµì‹œí‚¨ í”„ë¡¬í”„íŠ¸ ì ìš© ëª¨ë¸)\n","PATH_MODEL_TYPO = \"/content/drive/MyDrive/P01_Nandoc/Finalmodel1_3\"\n","\n","# [Core 2] ì•¼ë¯¼ì •ìŒ ë‹´ë‹¹\n","PATH_MODEL_YAMIN = \"/content/drive/MyDrive/P01_Nandoc/dict+jamoB+kobart_ft+cleanData/\"\n","\n","# ğŸ”¥ [ì¤‘ìš”] ì˜¤íƒ€ ëª¨ë¸ìš© í”„ë¡¬í”„íŠ¸ (í•™ìŠµ ì½”ë“œì—ì„œ ë°œì·Œ)\n","KOR_PROMPT = (\n","    \"ì•„ë˜ ì…ë ¥ì€ ë‚œë…í™”ëœ í•œêµ­ì–´ ë¬¸ì¥ì„ ìëª¨(ì´ˆì„±/ì¤‘ì„±/ì¢…ì„±)ë¡œ ë¶„í•´í•œ ë¬¸ìì—´ì´ì•¼.\\n\"\n","    \"ì´ ìëª¨ë“¤ì„ í•œêµ­ì–´ ë¬¸ì¥ ê·¸ëŒ€ë¡œ ë³µì›í•´.\\n\"\n",")\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"ğŸš€ Inference Device: {device}\")\n","\n","# =============================================================================\n","# 2. í†µí•© ë³µì› ì‹œìŠ¤í…œ í´ë˜ìŠ¤\n","# =============================================================================\n","class NandocSystem:\n","    def __init__(self):\n","        print(\"ğŸ”„ [System] ì—”ì§„ ì‹œë™ ì¤‘... (Prompt-Aware Mode)\")\n","        start_time = time.time()\n","\n","        # A. ë¶„ë¥˜ê¸° ë¡œë“œ\n","        try:\n","            self.cls_tokenizer = ElectraTokenizer.from_pretrained(PATH_CLASSIFIER)\n","            self.cls_model = ElectraForSequenceClassification.from_pretrained(PATH_CLASSIFIER).to(device)\n","            self.cls_model.eval()\n","            print(\"âœ… [1/3] ë¶„ë¥˜ê¸°(Classifier) ì¥ì°© ì™„ë£Œ\")\n","        except Exception as e:\n","            print(f\"âŒ ë¶„ë¥˜ê¸° ë¡œë“œ ì‹¤íŒ¨: {e}\")\n","            raise\n","\n","        # B. ì˜¤íƒ€ ëª¨ë¸ ë¡œë“œ (Prompt ì ìš© ëª¨ë¸)\n","        try:\n","            self.typo_tokenizer = PreTrainedTokenizerFast.from_pretrained(PATH_MODEL_TYPO)\n","            self.typo_model = BartForConditionalGeneration.from_pretrained(PATH_MODEL_TYPO).to(device)\n","            self.typo_model.eval()\n","            print(\"âœ… [2/3] ì˜¤íƒ€ ëª¨ë¸(Typo Core) ì¥ì°© ì™„ë£Œ\")\n","        except Exception as e:\n","            print(f\"âŒ ì˜¤íƒ€ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n","            raise\n","\n","        # C. ì•¼ë¯¼ ëª¨ë¸ ë¡œë“œ\n","        try:\n","            self.yamin_tokenizer = PreTrainedTokenizerFast.from_pretrained(PATH_MODEL_YAMIN)\n","            self.yamin_model = BartForConditionalGeneration.from_pretrained(PATH_MODEL_YAMIN).to(device)\n","            self.yamin_model.eval()\n","            print(\"âœ… [3/3] ì•¼ë¯¼ ëª¨ë¸(Yamin Core) ì¥ì°© ì™„ë£Œ\")\n","        except Exception as e:\n","            print(f\"âŒ ì•¼ë¯¼ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n","            raise\n","\n","        print(f\"ğŸ‰ [System] ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ ({time.time() - start_time:.2f}s)\\n\")\n","\n","    # -------------------------------------------------------------------------\n","    # ğŸ›¡ï¸ ë°©ì–´ ë¡œì§ 1: ì´ì¤‘ ì ê¸ˆ (Double Lock) -> ì˜¤íƒ€ ëª¨ë¸ìš© (ê°•ë ¥í•¨)\n","    # -------------------------------------------------------------------------\n","    def _double_lock_cut(self, input_text, pred_text):\n","        # 1. ë¬¸ì¥ ê°œìˆ˜ ë™ê¸°í™”\n","        input_punctuations = len(re.findall(r'[.!?]', input_text))\n","        target_count = max(input_punctuations, 1)\n","\n","        matches = [m.start() for m in re.finditer(r'[.!?]', pred_text)]\n","        if len(matches) >= target_count:\n","            cut_idx = matches[target_count - 1]\n","            candidate = pred_text[:cut_idx+1]\n","        else:\n","            candidate = pred_text\n","\n","        # 2. ê¸¸ì´ ì œí•œ (1.3ë°°)\n","        limit_len = int(len(input_text) * 1.3) + 10\n","        if len(candidate) > limit_len:\n","            truncated = candidate[:limit_len]\n","            # ì•ˆì „í•˜ê²Œ ìë¥´ê¸°\n","            last_punc = max(truncated.rfind('.'), truncated.rfind('!'), truncated.rfind('?'))\n","            if last_punc != -1: return truncated[:last_punc+1]\n","            return truncated\n","        return candidate\n","\n","    # -------------------------------------------------------------------------\n","    # ğŸ›¡ï¸ ë°©ì–´ ë¡œì§ 2: ë‹¨ìˆœ ì •ì œ (Simple Polish) -> ì•¼ë¯¼ ëª¨ë¸ìš© (ìœ ì—°í•¨)\n","    # -------------------------------------------------------------------------\n","    def _simple_defense(self, text):\n","        # 1. íŠ¹ìˆ˜ë¬¸ì ì²­ì†Œ (ê¸°ë³¸ì ì¸ ê²ƒë§Œ)\n","        clean_pattern = r'[^ê°€-í£a-zA-Z0-9\\s.,?!\\'\"%~]'\n","        text = re.sub(clean_pattern, '', text)\n","\n","        # 2. ë‹¤ì¤‘ ê³µë°± ì œê±°\n","        text = re.sub(r'\\s+', ' ', text).strip()\n","\n","        # 3. ì¤‘ë³µ ë¬¸ì¥ ì œê±° (ê°„ë‹¨ ë²„ì „)\n","        sentences = re.split(r'(?<=[.?!])\\s+', text)\n","        unique_sentences = []\n","        for s in sentences:\n","            s = s.strip()\n","            if not s: continue\n","            if unique_sentences and s == unique_sentences[-1]:\n","                continue\n","            unique_sentences.append(s)\n","        return ' '.join(unique_sentences)\n","\n","    # -------------------------------------------------------------------------\n","    # ğŸš€ ë©”ì¸ ì¶”ë¡  í•¨ìˆ˜ (Restore)\n","    # -------------------------------------------------------------------------\n","    def restore(self, text):\n","        if not text: return \"\"\n","\n","        # Step 1. ë¶„ë¥˜ (Classifier)\n","        cls_inputs = self.cls_tokenizer(\n","            text, return_tensors='pt', truncation=True, max_length=128\n","        ).to(device)\n","\n","        with torch.no_grad():\n","            outputs = self.cls_model(**cls_inputs)\n","            # 0: ì˜¤íƒ€/ì¼ë°˜, 1: ì•¼ë¯¼ì •ìŒ\n","            class_id = torch.argmax(outputs.logits, dim=-1).item()\n","            probs = torch.softmax(outputs.logits, dim=-1)[0]\n","            confidence = probs[class_id].item()\n","\n","        # Step 2. ë¼ìš°íŒ… & ìƒì„± (Routing & Generation)\n","\n","        # ====================================================\n","        # [Case A] ì˜¤íƒ€ ëª¨ë¸ (TYPE-0)\n","        # íŠ¹ì§•: í”„ë¡¬í”„íŠ¸ í•„ìš”, ê°•ë ¥í•œ ë°©ì–´ ë¡œì§ ì ìš©\n","        # ====================================================\n","        if class_id == 0:\n","            target_model = self.typo_model\n","            target_tokenizer = self.typo_tokenizer\n","            info = \"TYPE-0 (ì˜¤íƒ€)\"\n","\n","            # ğŸ”¥ [í•µì‹¬] í”„ë¡¬í”„íŠ¸ ê²°í•©\n","            final_input = KOR_PROMPT + text\n","\n","            gen_inputs = target_tokenizer(\n","                final_input, return_tensors='pt', padding=True, truncation=True, max_length=512 # í”„ë¡¬í”„íŠ¸ ë•Œë¬¸ì— ê¸¸ì´ ë„‰ë„‰íˆ\n","            ).to(device)\n","\n","            with torch.no_grad():\n","                gen_ids = target_model.generate(\n","                    gen_inputs['input_ids'],\n","                    attention_mask=gen_inputs['attention_mask'],\n","                    num_beams=5,\n","                    no_repeat_ngram_size=3,\n","                    repetition_penalty=1.5,\n","                    early_stopping=True,\n","                    max_length=256\n","                )\n","            raw_output = target_tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n","\n","            # ğŸ”¥ [í•µì‹¬] ê°•ë ¥í•œ ë°©ì–´ (Double Lock)\n","            processed_text = self._double_lock_cut(text, raw_output)\n","            final_text = self._simple_defense(processed_text) # ë§ˆë¬´ë¦¬ ì²­ì†Œ\n","\n","        # ====================================================\n","        # [Case B] ì•¼ë¯¼ ëª¨ë¸ (TYPE-1)\n","        # íŠ¹ì§•: í”„ë¡¬í”„íŠ¸ ì—†ìŒ, ìœ ì—°í•œ ë°©ì–´ ë¡œì§ ì ìš©\n","        # ====================================================\n","        else:\n","            target_model = self.yamin_model\n","            target_tokenizer = self.yamin_tokenizer\n","            info = \"TYPE-1 (ì•¼ë¯¼)\"\n","\n","            # ì›ë³¸ ê·¸ëŒ€ë¡œ ì…ë ¥\n","            final_input = text\n","\n","            gen_inputs = target_tokenizer(\n","                final_input, return_tensors='pt', padding=True, truncation=True, max_length=128\n","            ).to(device)\n","\n","            with torch.no_grad():\n","                gen_ids = target_model.generate(\n","                    gen_inputs['input_ids'],\n","                    attention_mask=gen_inputs['attention_mask'],\n","                    num_beams=5,\n","                    no_repeat_ngram_size=3,\n","                    repetition_penalty=1.5,\n","                    early_stopping=True,\n","                    max_length=256\n","                )\n","            raw_output = target_tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n","\n","            # ğŸ”¥ [í•µì‹¬] ê°„ë‹¨í•œ ë°©ì–´ (Simple Defense)\n","            final_text = self._simple_defense(raw_output)\n","\n","        return final_text, info, confidence\n"]},{"cell_type":"code","source":["\n","# =============================================================================\n","# 3. ì‹¤í–‰ í…ŒìŠ¤íŠ¸\n","# =============================================================================\n","if __name__ == \"__main__\":\n","    nandoc = NandocSystem()\n","\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"ğŸ’¡ [Nandoc Double-Engine] ê°€ë™ (Prompt: ON for Typo)\")\n","    print(\"=\"*60)\n","\n","    while True:\n","        user_input = input(\"\\nğŸ“ ì…ë ¥: \")\n","        if user_input.lower() in ['q', 'quit']:\n","            print(\"ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n","            break\n","\n","        if not user_input.strip(): continue\n","\n","        result, type_info, conf = nandoc.restore(user_input)\n","\n","        print(f\"   â””â”€ [{type_info} | {conf:.0%}] -> {result}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":801},"id":"N9qyoIsm4uof","executionInfo":{"status":"error","timestamp":1768451724002,"user_tz":-540,"elapsed":227,"user":{"displayName":"SiHOON KIM","userId":"10710758819479501067"}},"outputId":"251f2d40-77f5-4fd9-acfe-4071ad8db897"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ”„ [System] ì—”ì§„ ì‹œë™ ì¤‘... (Prompt-Aware Mode)\n","âœ… [1/3] ë¶„ë¥˜ê¸°(Classifier) ì¥ì°© ì™„ë£Œ\n","âŒ ì˜¤íƒ€ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: Can't load tokenizer for '/content/drive/MyDrive/P01_Nandoc/Finalmodel1_3'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/content/drive/MyDrive/P01_Nandoc/Finalmodel1_3' is the correct path to a directory containing all relevant files for a PreTrainedTokenizerFast tokenizer.\n"]},{"output_type":"error","ename":"OSError","evalue":"Can't load tokenizer for '/content/drive/MyDrive/P01_Nandoc/Finalmodel1_3'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/content/drive/MyDrive/P01_Nandoc/Finalmodel1_3' is the correct path to a directory containing all relevant files for a PreTrainedTokenizerFast tokenizer.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;31m# This is slightly better for only 1 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m             hf_hub_download(\n\u001b[0m\u001b[1;32m    480\u001b[0m                 \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;34m\"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/content/drive/MyDrive/P01_Nandoc/Finalmodel1_3'. Use `repo_type` argument if needed.","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1993\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1994\u001b[0;31m                         resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m   1995\u001b[0m                             \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m     \"\"\"\n\u001b[0;32m--> 322\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    531\u001b[0m         resolved_files = [\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0m_get_cache_file_to_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepo_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfull_filenames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36m_get_cache_file_to_return\u001b[0;34m(path_or_repo_id, full_filename, cache_dir, revision, repo_type)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;31m# We try to see if we have a cached version (not up to date):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     resolved_file = try_to_load_from_cache(\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepo_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepo_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;34m\"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/content/drive/MyDrive/P01_Nandoc/Finalmodel1_3'. Use `repo_type` argument if needed.","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-738901580.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# =============================================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mnandoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNandocSystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-596625001.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# B. ì˜¤íƒ€ ëª¨ë¸ ë¡œë“œ (Prompt ì ìš© ëª¨ë¸)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypo_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPreTrainedTokenizerFast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH_MODEL_TYPO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypo_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBartForConditionalGeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH_MODEL_TYPO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypo_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2012\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2013\u001b[0m                         \u001b[0;31m# For any other exception, we throw a generic error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2014\u001b[0;31m                         raise OSError(\n\u001b[0m\u001b[1;32m   2015\u001b[0m                             \u001b[0;34mf\"Can't load tokenizer for '{pretrained_model_name_or_path}'. If you were trying to load it from \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2016\u001b[0m                             \u001b[0;34m\"'https://huggingface.co/models', make sure you don't have a local directory with the same name. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for '/content/drive/MyDrive/P01_Nandoc/Finalmodel1_3'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/content/drive/MyDrive/P01_Nandoc/Finalmodel1_3' is the correct path to a directory containing all relevant files for a PreTrainedTokenizerFast tokenizer."]}]},{"cell_type":"code","source":["# =============================================================================\n","# 3. 5ê°œ ë¬¸ì¥ ìë™ í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n","# =============================================================================\n","if __name__ == \"__main__\":\n","    # 1. ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n","    nandoc = NandocSystem()\n","\n","    # 2. í…ŒìŠ¤íŠ¸ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ (ì˜¤íƒ€, ì•¼ë¯¼ì •ìŒ, í˜¼í•©)\n","    test_sentences = [\n","        \"ì•„... ê¹Œê»µ ì´ˆì½” ì€¼ë´ ë»¥ ëœ›ë ¤ì…” ì‹œì›í•˜ìë§Œ ë‹´ë°° ëƒ„ìŒ¢ ë°ë²„ë¦¼. ì‹¸ê²Œ í•˜ë£¨ë§Œ ë¬µê»¬ëŒœ! í•˜ëŠ” ìƒ¤ëŒí•œí…Œë§Œ ì¸„ì²œ. ë•¸ë¹¼ ëƒ„ìƒˆê¹Œ ëª¨íŠ¼ ì§±ì¡ˆì„ ì¹´ì³ê°¸ëŠ” ê¼¿. ë†ƒë˜ë¹µì—ì¨ ìº­ì¢… ëŒ¬ë¹¼ì™€ ìœ íì— ìªŒì˜€ì„ ë•Œ ë‚˜ëŠ” ëƒ„ìƒˆê¹Œ ê³˜ì‘‰ ë±Œì— ì´ì”€ ã…†... ì‹¸ë‹ˆê¹Œ í•  ë¨ˆ ì—†ìŒ.\",\n","        \"ì°Œì•ˆ ì¸…ì³ëŠšë£Œ ë”°ë„ˆì™”ëŠ”ë–¼ êº„ìµë¹  ì¬¬êµ ê¹”ë”í–ë„¤ìš”. ì©Œë ´í–”ë—– ë¬¼ì½° ìŒë£Œë˜¦ ì•˜ê¾œ ë¹µëš€ ì•„ì®¼ ë•¨ëœ»í–ˆìŠ´ë‚˜ë•¾. íœ˜ë‚™ì“°ë°™ê·¸ì™€ ê»´ëŸ¬ëš€ ë©¸ì € ì•Šì–†ì„œ ì¬¬í–¤ê¾¦ìš©. ëº˜ë¡¤ ì—¿ë²  í´ëŠ¬ì©Œë©ƒ ì–´ì ì  ë¼í–ˆì”€ë‚˜ë•¨. ë•®ìœ¼ë©”ë˜ ì•„ì˜¹í•  ì“¬ ì•…ì“°ë©´ ì—‹ìš©í–ê¾œ ìŒ‰ë„¤ìš”. ì˜Šì•„ê¹Œ ì–´ë µì°Œë¨„ ^^\",\n","        \"í˜¸í…”ì€ ê¹”ë”í•˜ê³  ì„œBsë„ ì¢‹ì•˜ëŠ”ë°, bangOl ì‘Oã… ì°½ë¬¸e ê²¨ìµ ì—†ì–´ì„œ í™˜71ã‰ r ì•ˆ ë˜ëŠ” ëŠë‚Œ0I 8ì—ˆì–´yO. ê·¸2Hã…Œ Oã…ì¹¨ ì‹€ã…†ëŠ” ì‹ ì„ ã…rê·¸ ë§›ìˆì—ˆìœ¼ni ë§Œì¡±^ëŸ¬ì› ìŠµL|ã…-.\",\n","        \"01ë²ˆ ì¦‰ë§O|| ì¹œãƒ²ë‘ goì„œ 1ë°• 2ì¼í–ˆì–´ìš”. bangOl ê¹”ë”ã…rãƒ± bã‰° ì „ë§0l ê²½ë§ ë©‹ì¡Œì–´8. ì¡°ì‹€E ê°„ë‹¨Haê²¨ë§Œ ë§›ìˆãƒ±, ì§ì›ë¶„dl0l ì¹œì ˆhriì„œ ë¶„ì¶71E ì¢‹ì•˜ì£ . ã…-ë§Œ, ìˆ˜ì˜ì¥ ì…7ã‰ r ì¡°ê¸ˆ ë¹½ë¹½ã…riì„œ ì¡°ê¸ˆ 71ã‰°ë ¸=ë°, ê·¸ ì™¸O||ëŠ” ì™„ì „ ë§Œì¡±sëŸ¬ìš´ oã…•í–‰0Iì—ˆì–´yO.\",\n","        \"í´ë‚œí•˜ ì¨œ ì“€ê¼¬ ì™”ì”€ë‚­ë•¿. ì­ˆìœ„ì— ë¨“ì©ëš” ë§ì•„ì  ì¬¬í” ì¼¯ êº„íƒ¸ìš”.\"\n","    ]\n","\n","    print(\"\\n\" + \"=\"*80)\n","    print(\"ğŸ“Š [Nandoc Intelligent System] 5ê°œ ë¬¸ì¥ ë³µì› í…ŒìŠ¤íŠ¸\")\n","    print(\"=\"*80)\n","\n","    for i, text in enumerate(test_sentences):\n","        # ë³µì› ìˆ˜í–‰\n","        result, type_info, conf = nandoc.restore(text)\n","\n","        print(f\"\\n[Test Case {i+1}]\")\n","        print(f\"ğŸ•µï¸â€â™‚ï¸ ê°ì§€ëœ íƒ€ì…: {type_info} (í™•ë¥ : {conf:.2%})\")\n","        print(f\"ğŸ“ ì›ë³¸: {text[:60]}...\" if len(text) > 60 else f\"ğŸ“ ì›ë³¸: {text}\")\n","        print(f\"âœ¨ ë³µì›: {result}\")\n","        print(\"-\" * 80)\n"],"metadata":{"id":"g-4PeNix48hu"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[],"authorship_tag":"ABX9TyMhHs5YTOAzZrILE9TXkBr8"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}