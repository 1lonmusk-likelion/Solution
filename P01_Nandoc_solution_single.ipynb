{"cells":[{"cell_type":"markdown","metadata":{"id":"rG6hH59LyLVU"},"source":["\n","\n","---\n","# ë‚œë…í™” ë³µì› ì†”ë£¨ì…˜: ì‹±ê¸€ ëª¨ë¸\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ry5hMfHlNATB","outputId":"446f9b68-aca4-43ab-fc65-5000be03d551","executionInfo":{"status":"ok","timestamp":1768451114941,"user_tz":-540,"elapsed":39225,"user":{"displayName":"SiHOON KIM","userId":"10710758819479501067"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2026.1.4)\n","Mounted at /content/drive\n","ğŸš€ Inference Device: cuda\n"]}],"source":["# =============================================================================\n","# 0. í™˜ê²½ ì„¤ì • (ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸)\n","# =============================================================================\n","!pip install transformers\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import time\n","import re\n","import torch\n","from transformers import ElectraTokenizer, ElectraForSequenceClassification\n","from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration\n","\n","# =============================================================================\n","# 1. ê²½ë¡œ ì„¤ì • (í•™ìŠµ ì™„ë£Œëœ ëª¨ë¸ ê²½ë¡œ)\n","# =============================================================================\n","# [Signal] ë¶„ë¥˜ê¸°: ì…ë ¥ì´ ì˜¤íƒ€ì¸ì§€ ì•¼ë¯¼ì •ìŒì¸ì§€ íŒë‹¨\n","PATH_CLASSIFIER = \"/content/drive/MyDrive/P01_Nandoc/Classifier\"\n","\n","# [Core] íŒŒì´ë„ ëª¨ë¸: íƒœê·¸ì— ë”°ë¼ ë³µì›ì„ ìˆ˜í–‰í•˜ëŠ” ë§ŒëŠ¥ ëª¨ë¸\n","PATH_FINAL_MODEL = \"/content/drive/MyDrive/P01_Nandoc/FINALV0\"\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"ğŸš€ Inference Device: {device}\")\n","\n","# =============================================================================\n","# 2. í†µí•© ë³µì› ì‹œìŠ¤í…œ í´ë˜ìŠ¤\n","# =============================================================================\n","class NandocSystem:\n","    def __init__(self):\n","        print(\"ğŸ”„ [System] ì—”ì§„ ì‹œë™ ì¤‘... (Intelligent Mode)\")\n","        start_time = time.time()\n","\n","        # A. ë¶„ë¥˜ê¸° ë¡œë“œ (KoELECTRA)\n","        try:\n","            self.cls_tokenizer = ElectraTokenizer.from_pretrained(PATH_CLASSIFIER)\n","            self.cls_model = ElectraForSequenceClassification.from_pretrained(PATH_CLASSIFIER).to(device)\n","            self.cls_model.eval()\n","            print(\"âœ… [1/2] ë¶„ë¥˜ê¸°(Signal) ì¥ì°© ì™„ë£Œ\")\n","        except Exception as e:\n","            print(f\"âŒ ë¶„ë¥˜ê¸° ë¡œë“œ ì‹¤íŒ¨: {e}\")\n","            raise\n","\n","        # B. íŒŒì´ë„ ëª¨ë¸ ë¡œë“œ (KoBART)\n","        try:\n","            self.tokenizer = PreTrainedTokenizerFast.from_pretrained(PATH_FINAL_MODEL)\n","            self.model = BartForConditionalGeneration.from_pretrained(PATH_FINAL_MODEL).to(device)\n","            self.model.eval()\n","            print(\"âœ… [2/2] íŒŒì´ë„ ëª¨ë¸(Core) ì¥ì°© ì™„ë£Œ\")\n","        except Exception as e:\n","            print(f\"âŒ íŒŒì´ë„ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n","            raise\n","\n","        print(f\"ğŸ‰ [System] ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ ({time.time() - start_time:.2f}s)\\n\")\n","\n","    # -------------------------------------------------------------------------\n","    # ğŸ”’ ë°©ì–´ ë¡œì§ (ì´ì¤‘ ì ê¸ˆ + ì •ì œ) - ë‡Œì ˆ ì›ì²œ ì°¨ë‹¨\n","    # -------------------------------------------------------------------------\n","    def _double_lock_cut(self, input_text, pred_text):\n","        # 1ì°¨: ë¬¸ì¥ ê°œìˆ˜ ë™ê¸°í™”\n","        input_punctuations = len(re.findall(r'[.!?]', input_text))\n","        target_count = max(input_punctuations, 1)\n","\n","        matches = [m.start() for m in re.finditer(r'[.!?]', pred_text)]\n","        if len(matches) >= target_count:\n","            cut_idx = matches[target_count - 1]\n","            candidate = pred_text[:cut_idx+1]\n","        else:\n","            candidate = pred_text\n","\n","        # 2ì°¨: ê¸¸ì´ ì œí•œ (1.3ë°°)\n","        limit_len = int(len(input_text) * 1.3) + 10\n","        if len(candidate) > limit_len:\n","            truncated = candidate[:limit_len]\n","            last_punc = max(truncated.rfind('.'), truncated.rfind('!'), truncated.rfind('?'))\n","            if last_punc != -1: return truncated[:last_punc+1]\n","            last_space = truncated.rfind(' ')\n","            if last_space != -1: return truncated[:last_space]\n","            return truncated\n","        return candidate\n","\n","    def _final_polish(self, text):\n","        # íŠ¹ìˆ˜ë¬¸ì ì œê±° & ì¤‘ë³µ ë¬¸ì¥ ì œê±°\n","        clean_pattern = r'[^ê°€-í£a-zA-Z0-9\\s.,?!\\'\"%~]'\n","        text = re.sub(clean_pattern, '', text)\n","        text = re.sub(r'\\s+', ' ', text).strip()\n","\n","        sentences = re.split(r'(?<=[.?!])\\s+', text)\n","        unique_sentences = []\n","        for s in sentences:\n","            s = s.strip()\n","            if not s: continue\n","            if unique_sentences and s == unique_sentences[-1]:\n","                continue\n","            unique_sentences.append(s)\n","        return ' '.join(unique_sentences)\n","\n","    # -------------------------------------------------------------------------\n","    # ğŸš€ ë©”ì¸ ì¶”ë¡  í•¨ìˆ˜ (Restore)\n","    # -------------------------------------------------------------------------\n","    def restore(self, text):\n","        if not text: return \"\"\n","\n","        # Step 1. ë¶„ë¥˜ (Classifier) -> ìƒí™© íŒë‹¨\n","        cls_inputs = self.cls_tokenizer(\n","            text, return_tensors='pt', truncation=True, max_length=128\n","        ).to(device)\n","\n","        with torch.no_grad():\n","            outputs = self.cls_model(**cls_inputs)\n","            # 0: ì˜¤íƒ€/ì¼ë°˜, 1: ì•¼ë¯¼ì •ìŒ\n","            class_id = torch.argmax(outputs.logits, dim=-1).item()\n","            probs = torch.softmax(outputs.logits, dim=-1)[0]\n","            confidence = probs[class_id].item()\n","\n","        # Step 2. íƒœê·¸ ë¶€ì°© (Tagging) -> ì‘ì—… ì§€ì‹œ\n","        # í•™ìŠµ ë°ì´í„°ì™€ ë™ì¼í•œ íƒœê·¸ë¥¼ ë¶™ì—¬ì•¼ ëª¨ë¸ì´ ì•Œì•„ë“£ìŠµë‹ˆë‹¤.\n","        if class_id == 0:\n","            tagged_text = f\"<ì˜¤ë¥˜> {text}\"   # ì˜¤íƒ€ ëª¨ë“œ ë°œë™\n","            info = \"TYPE-0 (ì˜¤íƒ€)\"\n","        else:\n","            tagged_text = f\"<ë…¸ì´ì¦ˆ> {text}\" # ì•¼ë¯¼ì •ìŒ ëª¨ë“œ ë°œë™\n","            info = \"TYPE-1 (ì•¼ë¯¼)\"\n","\n","        # Step 3. ë³µì› (Generation) -> íŒŒì´ë„ ëª¨ë¸ ìˆ˜í–‰\n","        gen_inputs = self.tokenizer(\n","            tagged_text, return_tensors='pt', padding=True, truncation=True, max_length=128\n","        ).to(device)\n","\n","        with torch.no_grad():\n","            gen_ids = self.model.generate(\n","                gen_inputs['input_ids'],\n","                attention_mask=gen_inputs['attention_mask'],\n","                num_beams=5,             # ì •í™•ë„ í–¥ìƒ\n","                no_repeat_ngram_size=3,  # ë°˜ë³µ ë°©ì§€\n","                repetition_penalty=1.5,  # ë‡Œì ˆ ì–µì œ\n","                early_stopping=True,\n","                max_length=256\n","            )\n","        raw_output = self.tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n","\n","        # Step 4. ë°©ì–´ (Defense) -> ê²°ê³¼ë¬¼ ë‹¤ë“¬ê¸°\n","        # ì£¼ì˜: ê¸¸ì´ ì œí•œì€ íƒœê·¸ ì—†ëŠ” 'ì›ë³¸ text' ê¸°ì¤€ìœ¼ë¡œ!\n","        cut_text = self._double_lock_cut(text, raw_output)\n","        final_text = self._final_polish(cut_text)\n","\n","        return final_text, info, confidence\n"]},{"cell_type":"code","source":["\n","# =============================================================================\n","# 3. ì‹¤í–‰ í…ŒìŠ¤íŠ¸\n","# =============================================================================\n","if __name__ == \"__main__\":\n","    # 1. ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n","    nandoc = NandocSystem()\n","\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"ğŸ’¡ [Nandoc Intelligent System] ê°€ë™ (ì¢…ë£Œ: 'q')\")\n","    print(\"=\"*60)\n","\n","    # 2. í…ŒìŠ¤íŠ¸ ë£¨í”„\n","    while True:\n","        user_input = input(\"\\nğŸ“ ì…ë ¥: \")\n","        if user_input.lower() in ['q', 'quit']:\n","            print(\"ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n","            break\n","\n","        if not user_input.strip(): continue\n","\n","        # ë³µì› ìˆ˜í–‰\n","        result, type_info, conf = nandoc.restore(user_input)\n","\n","        print(f\"   â””â”€ [{type_info} | {conf:.0%}] -> {result}\")"],"metadata":{"id":"A4IwG5Su2iXT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =============================================================================\n","# 3. 5ê°œ ë¬¸ì¥ ìë™ í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n","# =============================================================================\n","if __name__ == \"__main__\":\n","    # 1. ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n","    nandoc = NandocSystem()\n","\n","    # 2. í…ŒìŠ¤íŠ¸ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ (ì˜¤íƒ€, ì•¼ë¯¼ì •ìŒ, í˜¼í•©)\n","    test_sentences = [\n","        \"ì•„... ê¹Œê»µ ì´ˆì½” ì€¼ë´ ë»¥ ëœ›ë ¤ì…” ì‹œì›í•˜ìë§Œ ë‹´ë°° ëƒ„ìŒ¢ ë°ë²„ë¦¼. ì‹¸ê²Œ í•˜ë£¨ë§Œ ë¬µê»¬ëŒœ! í•˜ëŠ” ìƒ¤ëŒí•œí…Œë§Œ ì¸„ì²œ. ë•¸ë¹¼ ëƒ„ìƒˆê¹Œ ëª¨íŠ¼ ì§±ì¡ˆì„ ì¹´ì³ê°¸ëŠ” ê¼¿. ë†ƒë˜ë¹µì—ì¨ ìº­ì¢… ëŒ¬ë¹¼ì™€ ìœ íì— ìªŒì˜€ì„ ë•Œ ë‚˜ëŠ” ëƒ„ìƒˆê¹Œ ê³˜ì‘‰ ë±Œì— ì´ì”€ ã…†... ì‹¸ë‹ˆê¹Œ í•  ë¨ˆ ì—†ìŒ.\",\n","        \"ì°Œì•ˆ ì¸…ì³ëŠšë£Œ ë”°ë„ˆì™”ëŠ”ë–¼ êº„ìµë¹  ì¬¬êµ ê¹”ë”í–ë„¤ìš”. ì©Œë ´í–”ë—– ë¬¼ì½° ìŒë£Œë˜¦ ì•˜ê¾œ ë¹µëš€ ì•„ì®¼ ë•¨ëœ»í–ˆìŠ´ë‚˜ë•¾. íœ˜ë‚™ì“°ë°™ê·¸ì™€ ê»´ëŸ¬ëš€ ë©¸ì € ì•Šì–†ì„œ ì¬¬í–¤ê¾¦ìš©. ëº˜ë¡¤ ì—¿ë²  í´ëŠ¬ì©Œë©ƒ ì–´ì ì  ë¼í–ˆì”€ë‚˜ë•¨. ë•®ìœ¼ë©”ë˜ ì•„ì˜¹í•  ì“¬ ì•…ì“°ë©´ ì—‹ìš©í–ê¾œ ìŒ‰ë„¤ìš”. ì˜Šì•„ê¹Œ ì–´ë µì°Œë¨„ ^^\",\n","        \"í˜¸í…”ì€ ê¹”ë”í•˜ê³  ì„œBsë„ ì¢‹ì•˜ëŠ”ë°, bangOl ì‘Oã… ì°½ë¬¸e ê²¨ìµ ì—†ì–´ì„œ í™˜71ã‰ r ì•ˆ ë˜ëŠ” ëŠë‚Œ0I 8ì—ˆì–´yO. ê·¸2Hã…Œ Oã…ì¹¨ ì‹€ã…†ëŠ” ì‹ ì„ ã…rê·¸ ë§›ìˆì—ˆìœ¼ni ë§Œì¡±^ëŸ¬ì› ìŠµL|ã…-.\",\n","        \"01ë²ˆ ì¦‰ë§O|| ì¹œãƒ²ë‘ goì„œ 1ë°• 2ì¼í–ˆì–´ìš”. bangOl ê¹”ë”ã…rãƒ± bã‰° ì „ë§0l ê²½ë§ ë©‹ì¡Œì–´8. ì¡°ì‹€E ê°„ë‹¨Haê²¨ë§Œ ë§›ìˆãƒ±, ì§ì›ë¶„dl0l ì¹œì ˆhriì„œ ë¶„ì¶71E ì¢‹ì•˜ì£ . ã…-ë§Œ, ìˆ˜ì˜ì¥ ì…7ã‰ r ì¡°ê¸ˆ ë¹½ë¹½ã…riì„œ ì¡°ê¸ˆ 71ã‰°ë ¸=ë°, ê·¸ ì™¸O||ëŠ” ì™„ì „ ë§Œì¡±sëŸ¬ìš´ oã…•í–‰0Iì—ˆì–´yO.\",\n","        \"í´ë‚œí•˜ ì¨œ ì“€ê¼¬ ì™”ì”€ë‚­ë•¿. ì­ˆìœ„ì— ë¨“ì©ëš” ë§ì•„ì  ì¬¬í” ì¼¯ êº„íƒ¸ìš”.\"\n","    ]\n","\n","    print(\"\\n\" + \"=\"*80)\n","    print(\"ğŸ“Š [Nandoc Intelligent System] 5ê°œ ë¬¸ì¥ ë³µì› í…ŒìŠ¤íŠ¸\")\n","    print(\"=\"*80)\n","\n","    for i, text in enumerate(test_sentences):\n","        # ë³µì› ìˆ˜í–‰\n","        result, type_info, conf = nandoc.restore(text)\n","\n","        print(f\"\\n[Test Case {i+1}]\")\n","        print(f\"ğŸ•µï¸â€â™‚ï¸ ê°ì§€ëœ íƒ€ì…: {type_info} (í™•ë¥ : {conf:.2%})\")\n","        print(f\"ğŸ“ ì›ë³¸: {text[:60]}...\" if len(text) > 60 else f\"ğŸ“ ì›ë³¸: {text}\")\n","        print(f\"âœ¨ ë³µì›: {result}\")\n","        print(\"-\" * 80)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bb3r2mks44iS","executionInfo":{"status":"ok","timestamp":1768451771317,"user_tz":-540,"elapsed":12613,"user":{"displayName":"SiHOON KIM","userId":"10710758819479501067"}},"outputId":"e3086325-80ff-48f8-b256-b8ffd4ddd971"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ”„ [System] ì—”ì§„ ì‹œë™ ì¤‘... (Intelligent Mode)\n","âœ… [1/2] ë¶„ë¥˜ê¸°(Signal) ì¥ì°© ì™„ë£Œ\n"]},{"output_type":"stream","name":"stderr","text":["You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n"]},{"output_type":"stream","name":"stdout","text":["âœ… [2/2] íŒŒì´ë„ ëª¨ë¸(Core) ì¥ì°© ì™„ë£Œ\n","ğŸ‰ [System] ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ (0.71s)\n","\n","\n","================================================================================\n","ğŸ“Š [Nandoc Intelligent System] 5ê°œ ë¬¸ì¥ ë³µì› í…ŒìŠ¤íŠ¸\n","================================================================================\n","\n","[Test Case 1]\n","ğŸ•µï¸â€â™‚ï¸ ê°ì§€ëœ íƒ€ì…: TYPE-0 (ì˜¤íƒ€) (í™•ë¥ : 99.98%)\n","ğŸ“ ì›ë³¸: ì•„... ê¹Œê»µ ì´ˆì½” ì€¼ë´ ë»¥ ëœ›ë ¤ì…” ì‹œì›í•˜ìë§Œ ë‹´ë°° ëƒ„ìŒ¢ ë°ë²„ë¦¼. ì‹¸ê²Œ í•˜ë£¨ë§Œ ë¬µê»¬ëŒœ! í•˜ëŠ” ìƒ¤ëŒí•œí…Œë§Œ ì¸„ì²œ...\n","âœ¨ ë³µì›: ì•„... ê°€ê²© ì¢‹ê³  ë·°ë„ ë»¥ ëš«ë ¤ì„œ ì‹œì›í•˜ì§€ë§Œ ë‹´ë°° ëƒ„ìƒˆ ë¯¸ì³ë²„ë¦¼. ì‹¸ê²Œ í•˜ë£¨ë§Œ ë¬µê² ë‹¤! í•˜ëŠ” ì‚¬ëŒí•œí…Œë§Œ ì¶”ì²œ. ë‹´ë°° ëƒ„ìƒˆê°€ ëª¨ë“  ì¥ì ì„ ê°€ì ¸ê°€ëŠ” ê³³. ë…¸ë˜ë°©ì—ì„œ ê°ì¢… ë‹´ë°°ì™€ ìœ í¥ì— ì©”ì—ˆì„ ë•Œ ë‚˜ëŠ” ëƒ„ìƒˆê°€ ê³„ì† ë°©ì— ìˆìŒ ... ì‹¸ë‹ˆê¹Œ í•  ë§ ì—†ìŒ.\n","--------------------------------------------------------------------------------\n","\n","[Test Case 2]\n","ğŸ•µï¸â€â™‚ï¸ ê°ì§€ëœ íƒ€ì…: TYPE-0 (ì˜¤íƒ€) (í™•ë¥ : 99.98%)\n","ğŸ“ ì›ë³¸: ì°Œì•ˆ ì¸…ì³ëŠšë£Œ ë”°ë„ˆì™”ëŠ”ë–¼ êº„ìµë¹  ì¬¬êµ ê¹”ë”í–ë„¤ìš”. ì©Œë ´í–”ë—– ë¬¼ì½° ìŒë£Œë˜¦ ì•˜ê¾œ ë¹µëš€ ì•„ì®¼ ë•¨ëœ»í–ˆìŠ´ë‚˜ë•¾. íœ˜ë‚™ì“°...\n","âœ¨ ë³µì›: ì§€ì¸ ì¶”ì²œìœ¼ë¡œ ë‹¤ë…€ì™”ëŠ”ë° ê°€ì„±ë¹„ ì¢‹ê³  ê¹”ë”í•˜ë„¤ìš”. ì €ë ´í•œë° ë¬¼ê³¼ ìŒë£Œë„ ìˆê³  ë°©ë„ ìˆì–´ ë”°ëœ»í–ˆìŠµë‹ˆë‹¤. íœ˜ë‹‰ìŠ¤íŒŒí¬ì™€ ê±°ë¦¬ë„ ë©€ì§€ ì•Šì•„ì„œ ì¢‹ì•˜ì–´ìš”. ë°”ë¡œ ì˜†ì— í¸ì˜ì ë„ ìˆì–´ì„œ í¸ë¦¬í–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒì—ë„ ì´ìš©í•  ìˆ˜ ìˆìœ¼ë©´ ì´ìš©í•˜ê³  ì‹¶ë„¤ìš”.\n","--------------------------------------------------------------------------------\n","\n","[Test Case 3]\n","ğŸ•µï¸â€â™‚ï¸ ê°ì§€ëœ íƒ€ì…: TYPE-1 (ì•¼ë¯¼) (í™•ë¥ : 99.98%)\n","ğŸ“ ì›ë³¸: í˜¸í…”ì€ ê¹”ë”í•˜ê³  ì„œBsë„ ì¢‹ì•˜ëŠ”ë°, bangOl ì‘Oã… ì°½ë¬¸e ê²¨ìµ ì—†ì–´ì„œ í™˜71ã‰ r ì•ˆ ë˜ëŠ” ëŠë‚Œ0I 8ì—ˆ...\n","âœ¨ ë³µì›: í˜¸í…”ì€ ê¹”ë”í•˜ê³  ì„œë¹„ìŠ¤ë„ ì¢‹ì•˜ëŠ”ë°, ë°©ì´ ì‘ì•„ ì°½ë¬¸ì´ ê±°ì˜ ì—†ì–´ì„œ í™˜ê¸°ê°€ ì•ˆ ë˜ëŠ” ëŠë‚Œì´ ë“¤ì—ˆì–´ìš”. ê·¸ë˜ë„ ì•„ì¹¨ ì‹ì‚¬ëŠ” ì‹ ì„ í•˜ê³  ë§›ìˆì—ˆìœ¼ë‹ˆ ë§Œì¡±ìŠ¤ëŸ¬ì› ìŠµë‹ˆë‹¤.\n","--------------------------------------------------------------------------------\n","\n","[Test Case 4]\n","ğŸ•µï¸â€â™‚ï¸ ê°ì§€ëœ íƒ€ì…: TYPE-1 (ì•¼ë¯¼) (í™•ë¥ : 99.98%)\n","ğŸ“ ì›ë³¸: 01ë²ˆ ì¦‰ë§O|| ì¹œãƒ²ë‘ goì„œ 1ë°• 2ì¼í–ˆì–´ìš”. bangOl ê¹”ë”ã…rãƒ± bã‰° ì „ë§0l ê²½ë§ ë©‹ì¡Œì–´8. ì¡°ì‹€...\n","âœ¨ ë³µì›: ì´ë²ˆ ì£¼ë§ì— ì¹œêµ¬ë‘ ê°€ì„œ 1ë°• 2ì¼í–ˆì–´ìš”. ë°©ì´ ê¹”ë”í•˜ê³  ë°”ë‹¤ ì „ë§ì´ ì •ë§ ë©‹ì¡Œì–´ìš”. ì¡°ì‹ë„ ê°„ë‹¨í•˜ì§€ë§Œ ë§›ìˆê³ , ì§ì›ë¶„ë“¤ì´ ì¹œì ˆí•´ì„œ ë¶„ìœ„ê¸°ë„ ì¢‹ì•˜ì£ . ë‹¤ë§Œ, ìˆ˜ì˜ì¥ ì…êµ¬ê°€ ì¡°ê¸ˆ ë¹½ë¹½í•´ì„œ ì¡°ê¸ˆ ê¸°ë‹¤ë ¸ëŠ”ë°, ê·¸ ì™¸ì—ëŠ” ì™„ì „ ë§Œì¡±ìŠ¤ëŸ¬ìš´ ì—¬í–‰ì´ì—ˆì–´ìš”.\n","--------------------------------------------------------------------------------\n","\n","[Test Case 5]\n","ğŸ•µï¸â€â™‚ï¸ ê°ì§€ëœ íƒ€ì…: TYPE-0 (ì˜¤íƒ€) (í™•ë¥ : 99.98%)\n","ğŸ“ ì›ë³¸: í´ë‚œí•˜ ì¨œ ì“€ê¼¬ ì™”ì”€ë‚­ë•¿. ì­ˆìœ„ì— ë¨“ì©ëš” ë§ì•„ì  ì¬¬í” ì¼¯ êº„íƒ¸ìš”.\n","âœ¨ ë³µì›: í¸ì•ˆíˆ ì˜ ì‰¬ê³  ì™”ìŠµë‹ˆë‹¤. ì£¼ìœ„ì— ë§›ì§‘ë„ ë§ì•„ì„œ ì¢‹ì€ ê²ƒ ê°™ì•„ìš”.\n","--------------------------------------------------------------------------------\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[],"authorship_tag":"ABX9TyNzG3KAjchaHmsQb39uynoW"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}